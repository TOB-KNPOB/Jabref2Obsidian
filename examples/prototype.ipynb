{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root folder of the project to path\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.bib` entries loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n",
      "Entry type software not standard. Not considered.\n"
     ]
    }
   ],
   "source": [
    "import bibtexparser\n",
    "\n",
    "with open('../data/Knowledge.bib') as bibtex_file:\n",
    "    bib_database = bibtexparser.load(bibtex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, Yue and Chen, Lihua and Yick, Kit-lun and Yu, Winnie and Lau, Newman and Jiao, Wanzhong\n",
      "The FE model was supervised only by one variable: the increment of nipple points distance, which seems to be a very weak constraint. Would the whole workflow work when introducing all points tracked by the motion capture system, or even the ultra-dense motion capture system?\n"
     ]
    }
   ],
   "source": [
    "print(bib_database.entries[3]['author'])\n",
    "print(bib_database.entries[3]['comment'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group tree parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_group_tree(tree_str: str) -> dict:\n",
    "    # extract group level and name and combined as an order list\n",
    "    lines = re.split('\\n', tree_str)\n",
    "    group_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Group' in line:\n",
    "            level = int(line[0])\n",
    "            if level == 0:\n",
    "                group_list.append({\n",
    "                    'level': level,\n",
    "                    'name': 'Root',\n",
    "                    'node': {'entries': []},\n",
    "                    })\n",
    "                \n",
    "            else:\n",
    "                name = re.search(r\"(?<=Group:)[^\\\\;]+\", line).group()  # at current stage only english group names are supported\n",
    "                group_list.append({\n",
    "                    'level': level, \n",
    "                    'name': name,\n",
    "                    'node': {'entries': []},\n",
    "                    })\n",
    "\n",
    "    # prepare a dictionary of all groups for fast accessing via group name\n",
    "    group_dict = {}\n",
    "\n",
    "    for group in group_list:\n",
    "        group_dict[group['name']] = group['node']\n",
    "\n",
    "    # parse the group tree\n",
    "    for idx_current in range(len(group_list)):\n",
    "        # search backward for the 1st higher level node as current node's source node\n",
    "        for idx_previous in range(idx_current, -1, -1):\n",
    "            if group_list[idx_previous]['level'] < group_list[idx_current]['level']:\n",
    "                source_node = group_list[idx_previous]['node']\n",
    "                current_node = group_list[idx_current]['node']\n",
    "                current_name = group_list[idx_current]['name']\n",
    "                source_node[current_name] = current_node\n",
    "                break\n",
    "    \n",
    "    tree_root = group_list[0]['node']\n",
    "\n",
    "    return group_dict, tree_root\n",
    "\n",
    "tree_str = bib_database.comments[1]\n",
    "group_dict, tree_root = parse_group_tree(tree_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach each group with the corresponding entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_entries_to_group(entries: list, group_dict: dict):\n",
    "    for entry in entries:\n",
    "        # if the entry has groups attr, attach it to the corresponding groups\n",
    "        if 'groups' in entry.keys():\n",
    "            groups = re.split(', ', entry['groups'])\n",
    "\n",
    "            for group in groups:\n",
    "                group_dict[group]['entries'].append(entry)\n",
    "\n",
    "        # if the entry doesn't have groups attr, attach it to the root group\n",
    "        else:\n",
    "            group_dict['Root']['entries'].append(entry)\n",
    "\n",
    "attach_entries_to_group(bib_database.entries, group_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate group pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def export_group_pages(node: dict, name: str, source_name: str = \"None\", export_folder: str = 'output/'):\n",
    "    # create group folder\n",
    "    if os.path.exists(export_folder):\n",
    "        shutil.rmtree(export_folder)\n",
    "    os.mkdir(export_folder)\n",
    "\n",
    "    # create group's markdown page\n",
    "    md_path = os.path.join(export_folder, '{}.md'.format(name))\n",
    "            \n",
    "    with open(md_path, 'w') as file:\n",
    "        # title section\n",
    "        file.write(\"# {}\\n\".format(name))\n",
    "\n",
    "        # roadmap section\n",
    "        file.write(\"---\\n\")\n",
    "        file.write(\"#### Roadmap\\n\\n\")\n",
    "\n",
    "        if name is not None:\n",
    "            file.write(\"Source group: [[{}]]\\n\".format(source_name))\n",
    "\n",
    "        file.write(\"Subgroups: \\n\\n\")\n",
    "\n",
    "        for subgroup in node.keys():\n",
    "            if subgroup != \"entries\":\n",
    "                file.write(\"- [[{}]]\\n\".format(subgroup))\n",
    "\n",
    "        # entries section\n",
    "        file.write(\"---\\n\")\n",
    "        file.write(\"#### Entries\\n\\n\")\n",
    "        file.write(\"| Type | Note | Title |\\n\")\n",
    "        file.write(\"| --- | --- | --- |\\n\")\n",
    "        \n",
    "        for entry in node['entries']:\n",
    "            try:\n",
    "                file.write(\"| {} | [[{}]] | {} |\\n\".format(\n",
    "                entry['ENTRYTYPE'],\n",
    "                entry['ID'],\n",
    "                entry['title'],\n",
    "                ))\n",
    "            except:\n",
    "                print(\"entry information miss:\")\n",
    "                print(entry)\n",
    "\n",
    "    # recurse to the subgroups\n",
    "    for subgroup in node.keys():\n",
    "        if subgroup != \"entries\":\n",
    "            sub_folder = os.path.join(export_folder, subgroup)\n",
    "            export_group_pages(node[subgroup], subgroup, name, sub_folder)\n",
    "\n",
    "export_group_pages(node=tree_root, name='Root', export_folder='../output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c8ccae62b03e03f602162ec2d3ff7715f4331cfb18cae8be1463dcdd08d9f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
